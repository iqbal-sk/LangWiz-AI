{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "data = pd.read_csv('en-fr.csv')\n",
    "data = data[data['source_language'].isin(['en'])]\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-07T12:08:20.833312Z",
     "start_time": "2024-07-07T12:08:18.895181Z"
    }
   },
   "id": "3cac165e2cbe0e7f",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "# Initialize tokenizer for English\n",
    "tokenizer_en = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer_en.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer_en = trainers.WordPieceTrainer(vocab_size=6000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "tokenizer_en.train_from_iterator(train_data['source'].tolist(), trainer_en)\n",
    "\n",
    "# Initialize tokenizer for French\n",
    "tokenizer_fr = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer_fr.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer_fr = trainers.WordPieceTrainer(vocab_size=6000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "tokenizer_fr.train_from_iterator(train_data['reference'].tolist(), trainer_fr)\n",
    "\n",
    "# Save the tokenizers\n",
    "tokenizer_en.save(\"tokenizer_en.json\")\n",
    "tokenizer_fr.save(\"tokenizer_fr.json\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-07T12:08:21.695591Z",
     "start_time": "2024-07-07T12:08:21.465690Z"
    }
   },
   "id": "a90d7e5c17a515ed",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def encode_and_pad(tokenizer, texts, max_len):\n",
    "    # Encode texts\n",
    "    encoded_texts = [tokenizer.encode(text).ids for text in texts]\n",
    "    \n",
    "    # Pad encoded texts\n",
    "    padded_texts = [seq + [tokenizer.token_to_id(\"[PAD]\")] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in encoded_texts]\n",
    "    return padded_texts\n",
    "\n",
    "max_length = 50  # Define the maximum sequence length\n",
    "train_data['source_encoded'] = encode_and_pad(tokenizer_en, train_data['source'], max_length)\n",
    "train_data['target_encoded'] = encode_and_pad(tokenizer_fr, train_data['reference'], max_length)\n",
    "val_data['source_encoded'] = encode_and_pad(tokenizer_en, val_data['source'], max_length)\n",
    "val_data['target_encoded'] = encode_and_pad(tokenizer_fr, val_data['reference'], max_length)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-07T12:08:22.925200Z",
     "start_time": "2024-07-07T12:08:22.671229Z"
    }
   },
   "id": "14b3a540ef38203e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, tgt_data):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src = torch.tensor(self.src_data[index])\n",
    "        tgt = torch.tensor(self.tgt_data[index])\n",
    "        return src, tgt\n",
    "\n",
    "train_dataset = TranslationDataset(train_data['source_encoded'].tolist(), train_data['target_encoded'].tolist())\n",
    "val_dataset = TranslationDataset(val_data['source_encoded'].tolist(), val_data['target_encoded'].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-07T12:08:29.877041Z",
     "start_time": "2024-07-07T12:08:29.871813Z"
    }
   },
   "id": "c49f1927005a2b69",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Seq2SeqGRU(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(Seq2SeqGRU, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
    "        self.encoder = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.decoder = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embedded_src = self.encoder_embedding(src)\n",
    "        embedded_tgt = self.decoder_embedding(tgt)\n",
    "        _, hidden = self.encoder(embedded_src)\n",
    "        output, _ = self.decoder(embedded_tgt, hidden)\n",
    "        return self.out(output)\n",
    "    \n",
    "    \n",
    "src_vocab_size = 6000\n",
    "tgt_vocab_size = 6000\n",
    "embed_dim = 128  # Size of the embeddings\n",
    "hidden_dim = 256\n",
    "n_layers = 3\n",
    "\n",
    "model = Seq2SeqGRU(src_vocab_size, tgt_vocab_size, embed_dim, hidden_dim, n_layers)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_fr.token_to_id(\"[PAD]\"))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-07T12:08:32.915751Z",
     "start_time": "2024-07-07T12:08:32.285549Z"
    }
   },
   "id": "ad67371d704e71c4",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 5.669390930175782, Val Loss: 5.401816219091415\n",
      "Epoch 2, Train Loss: 5.042625526428223, Val Loss: 4.804058834910393\n",
      "Epoch 3, Train Loss: 4.331138456344605, Val Loss: 4.089646831154823\n",
      "Epoch 4, Train Loss: 3.559778356552124, Val Loss: 3.392179347574711\n",
      "Epoch 5, Train Loss: 2.8656258583068848, Val Loss: 2.880200032144785\n",
      "Epoch 6, Train Loss: 2.3665108690261842, Val Loss: 2.5587290078401566\n",
      "Epoch 7, Train Loss: 1.9911110172271729, Val Loss: 2.34603931568563\n",
      "Epoch 8, Train Loss: 1.702771348953247, Val Loss: 2.1865750085562468\n",
      "Epoch 9, Train Loss: 1.4552901000976564, Val Loss: 2.0931572504341602\n",
      "Epoch 10, Train Loss: 1.2492290813922882, Val Loss: 2.024342691525817\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.long(), tgt.long()  # Ensure inputs are in long format for embeddings\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src, tgt[:, :-1])\n",
    "            loss = criterion(outputs.view(-1, tgt_vocab_size), tgt[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                # print(src.shape, tgt.shape)\n",
    "                src, tgt = src.long(), tgt.long()  # Ensure inputs are in long format for embeddings\n",
    "                outputs = model(src, tgt[:, :-1])\n",
    "                loss = criterion(outputs.view(-1, tgt_vocab_size), tgt[:, 1:].reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, n_epochs=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-07T12:07:37.106492Z",
     "start_time": "2024-07-07T12:00:35.438793Z"
    }
   },
   "id": "6396395d918ccbf4",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c48e4e8c85032dd8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
