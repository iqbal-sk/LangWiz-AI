{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:12:32.453403Z",
     "start_time": "2024-07-27T16:12:31.601365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                         source_text  \\\n0                                           Bonjour,   \n1  Mes livres #PRS_ORG# empruntés restent toujour...   \n2  Ça ne fonctionne pas et ça fait plusieurs fois...   \n3                                             Merci.   \n4                                            #EMAIL#   \n\n                                         target_text  \n0                                             Hello,  \n1  My borrowed #PRS_ORG# books are still protecte...  \n2  It doesn't work and I've been through this pro...  \n3                                         Thank you.  \n4                                            #EMAIL#  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_text</th>\n      <th>target_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bonjour,</td>\n      <td>Hello,</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mes livres #PRS_ORG# empruntés restent toujour...</td>\n      <td>My borrowed #PRS_ORG# books are still protecte...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ça ne fonctionne pas et ça fait plusieurs fois...</td>\n      <td>It doesn't work and I've been through this pro...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Merci.</td>\n      <td>Thank you.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#EMAIL#</td>\n      <td>#EMAIL#</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('../artifacts/data/wmt/processed/train/en-fr.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3b7bfb77c62d346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:12:39.368439Z",
     "start_time": "2024-07-27T16:12:39.364212Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b8c10e00a6f61b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:12:43.762280Z",
     "start_time": "2024-07-27T16:12:40.951361Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer\n",
    "model_token = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_token,return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iqbal/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4144: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[10648,   383,  2949,     0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "        encodings = tokenizer('hi bub', max_length = 128,\n",
    "                                         truncation=True, padding=True,\n",
    "                                         return_tensors = 'pt')\n",
    "        \n",
    "encodings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T16:12:44.285204Z",
     "start_time": "2024-07-27T16:12:44.278409Z"
    }
   },
   "id": "95524c300c0add59",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[10648,   383,  2949,     0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text_target='hi bub', max_length = 128,\n",
    "                                         truncation=True, padding=True,\n",
    "                                         return_tensors = 'pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T16:12:44.911648Z",
     "start_time": "2024-07-27T16:12:44.907369Z"
    }
   },
   "id": "1dd4f1816ae0cff0",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5debc553a9316f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:17:48.617969Z",
     "start_time": "2024-07-27T16:17:48.613614Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length=128\n",
    "\n",
    "def preprocess_function(example):\n",
    "        # print(example)\n",
    "        try:\n",
    "            inputs = example['source_text']\n",
    "            targets = example['target_text']\n",
    "            \n",
    "            if not isinstance(inputs,(str,list)):\n",
    "                inputs = str(inputs)\n",
    "            if not isinstance(targets,(str,list)):\n",
    "                targets = str(targets)\n",
    "            \n",
    "            input_encodings = tokenizer(inputs, max_length = max_length,\n",
    "                                             truncation=True, padding=True,\n",
    "                                             return_tensors = 'pt')\n",
    "            \n",
    "            target_encodings= tokenizer(max_length = max_length,\n",
    "                                             truncation=True, padding=True,\n",
    "                                             return_tensors = 'pt', text_target=targets)\n",
    "            \n",
    "            return {\n",
    "                'input_ids' : input_encodings['input_ids'],\n",
    "                'attention_mask' : input_encodings['attention_mask'],\n",
    "                'labels': target_encodings['input_ids']       \n",
    "            }\n",
    "        except Exception as e:\n",
    "            print('---------------------------------')\n",
    "            print(e)\n",
    "            print(example)\n",
    "            print(type(example['source_text']), type(example['target_text']))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/15025 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc1b96d3cccb424a9d0b4f82ef26e6ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "data = load_dataset('csv', data_files='../artifacts/data/wmt/processed/train/en-fr.csv')\n",
    "data_pt = data.map(preprocess_function, batched=True)\n",
    "data_pt.save_to_disk('./processed_dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T16:17:50.446523Z",
     "start_time": "2024-07-27T16:17:49.403354Z"
    }
   },
   "id": "9f666bd6f58e66f2",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "model_fr_en = AutoModelForSeq2SeqLM.from_pretrained(model_token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T16:17:57.633414Z",
     "start_time": "2024-07-27T16:17:54.572400Z"
    }
   },
   "id": "dda9ef441d8e9329",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer,model = model_fr_en)\n",
    "        \n",
    "data_pt = load_from_disk('./processed_dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T16:17:58.494829Z",
     "start_time": "2024-07-27T16:17:58.485805Z"
    }
   },
   "id": "e637974144900d82",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T16:17:59.112915Z",
     "start_time": "2024-07-27T16:17:59.110309Z"
    }
   },
   "id": "50cb22bb6610af3b",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='17' max='1410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  17/1410 00:11 < 17:19, 1.34 it/s, Epoch 0.03/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 16\u001B[0m\n\u001B[1;32m      1\u001B[0m trainer_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m      2\u001B[0m             output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.output\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      3\u001B[0m             num_train_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m             weight_decay\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.02\u001B[39m\n\u001B[1;32m      8\u001B[0m         )\n\u001B[1;32m     10\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     11\u001B[0m             model \u001B[38;5;241m=\u001B[39m model_fr_en, args \u001B[38;5;241m=\u001B[39m trainer_args,\n\u001B[1;32m     12\u001B[0m             tokenizer \u001B[38;5;241m=\u001B[39m tokenizer, data_collator \u001B[38;5;241m=\u001B[39m seq2seq_data_collator,\n\u001B[1;32m     13\u001B[0m             train_dataset \u001B[38;5;241m=\u001B[39m data_pt[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     14\u001B[0m             \n\u001B[1;32m     15\u001B[0m         )\n\u001B[0;32m---> 16\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/transformers/trainer.py:1938\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1936\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   1937\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1938\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1939\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1940\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1941\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1942\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1943\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/transformers/trainer.py:2279\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2276\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   2278\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 2279\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2282\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2283\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2284\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2285\u001B[0m ):\n\u001B[1;32m   2286\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2287\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/transformers/trainer.py:3349\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   3347\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m   3348\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3349\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3351\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/accelerate/accelerator.py:2159\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   2157\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[1;32m   2158\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2159\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer_args = TrainingArguments(\n",
    "            output_dir='.output',\n",
    "            num_train_epochs = 3,\n",
    "            learning_rate = 2e-5,\n",
    "            per_device_train_batch_size = 32,\n",
    "            per_device_eval_batch_size= 32,\n",
    "            weight_decay= 0.02\n",
    "        )\n",
    "        \n",
    "trainer = Trainer(\n",
    "            model = model_fr_en, args = trainer_args,\n",
    "            tokenizer = tokenizer, data_collator = seq2seq_data_collator,\n",
    "            train_dataset = data_pt['train'],\n",
    "            \n",
    "        )\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T16:18:18.162827Z",
     "start_time": "2024-07-27T16:17:59.793496Z"
    }
   },
   "id": "ef267a51b3a54660",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configured till above"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69bd274708a39f9c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3dc9cac840545cb8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5517ec165fb11c34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e7af0136071560c3"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c2f8a4d103792ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:42:20.799636Z",
     "start_time": "2024-07-27T14:42:17.475548Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0beb75d874dbc29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:48:22.454724Z",
     "start_time": "2024-07-27T14:48:22.452261Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader=DataLoader(tokenized_datasets,shuffle =True,collate_fn = data_collator,batch_size=8)\n",
    "eval_dataloader=DataLoader(test_datasets,collate_fn = data_collator,batch_size=8)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:48:25.369092Z",
     "start_time": "2024-07-27T14:48:25.365565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader"
   ],
   "id": "ecb40cf96ace9fe9",
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataloader.DataLoader at 0x30c640710>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:48:28.828409Z",
     "start_time": "2024-07-27T14:48:26.070824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_token)"
   ],
   "id": "f1c5d3cc000aace4",
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "260f298b2597c254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:48:29.627175Z",
     "start_time": "2024-07-27T14:48:29.624329Z"
    }
   },
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "a895e35c2cbf3e4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:48:30.859688Z",
     "start_time": "2024-07-27T14:48:30.238311Z"
    }
   },
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model,optimizer,train_dataloader,eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "84ea1daf36ee33d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:48:31.266506Z",
     "start_time": "2024-07-27T14:48:31.263728Z"
    }
   },
   "source": [
    "from transformers import get_scheduler\n",
    "num_train_epochs =3\n",
    "num_steps = len(train_dataloader)\n",
    "num_train_steps = num_train_epochs * num_steps\n",
    "\n",
    "lr_scheduler = get_scheduler('linear', optimizer =optimizer,num_warmup_steps =0,num_training_steps=num_train_steps)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "f68be656a043cec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:48:32.036848Z",
     "start_time": "2024-07-27T14:48:32.033834Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "43ccd41ca2c2dd5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:48:32.615197Z",
     "start_time": "2024-07-27T14:48:32.613317Z"
    }
   },
   "source": [
    "output_dir = './saved_model/'"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "d7069a2e23e05e25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:48:33.263159Z",
     "start_time": "2024-07-27T14:48:33.177412Z"
    }
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_train_steps))\n",
    "\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4509 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d01fa0585f74f2f805c5f5ae228c4d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 11\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_train_epochs):\n\u001B[1;32m      8\u001B[0m     \n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# Training\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 11\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/accelerate/data_loader.py:454\u001B[0m, in \u001B[0;36mDataLoaderShard.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001B[39;00m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 454\u001B[0m     current_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    205\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:127\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 127\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type(\u001B[43m{\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43melem\u001B[49m\u001B[43m}\u001B[49m)\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:127\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 127\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type({key: \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem})\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[0;32m~/Desktop/LangWiz-AI/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:138\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    136\u001B[0m elem_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mnext\u001B[39m(it))\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28mlen\u001B[39m(elem) \u001B[38;5;241m==\u001B[39m elem_size \u001B[38;5;28;01mfor\u001B[39;00m elem \u001B[38;5;129;01min\u001B[39;00m it):\n\u001B[0;32m--> 138\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meach element in list of batch should be of equal size\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n",
      "\u001B[0;31mRuntimeError\u001B[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
